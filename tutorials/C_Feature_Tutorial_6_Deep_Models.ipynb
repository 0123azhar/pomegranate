{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Neural Networks\n",
    "\n",
    "author: Jacob Schreiber <br>\n",
    "contact: jmschreiber91@gmail.com\n",
    "\n",
    "Neural networks have become exceedingly popular recently due, in part, to their ability to achieve state-of-the-art performance on a variety of tasks without requiring complicated feature extraction pipelines. These models are frequently applied to domains where there is a great deal of raw structured data, such as computer vision, where neighboring pixels are strongly correlated, and natural language processing, where words are organized and modified in specific ways to convey meaning.\n",
    "\n",
    "There is some overlap between neural networks and probabilistic models. For example, deep hidden Markov models (DHMMs) are models where the input to the neural network is some observation, such as an image, and the output is the state in the hidden Markov model that the observation belongs to. These resulting probabilities are then treated as the likelihood function P(D|M) by the model, regularized using the transition matrix, and then re-normalized to get the posterior probabilities. Another example is a deep mixture model, where expectation-maximization is used to train the model on unlabeled images.\n",
    "\n",
    "Thus far, pomegranate has stuck to probabilistic models that are not coupled with a neural network. However, with the recent inclusion of custom distributions, one can use a quick hack in order to turn many of pomegranate's models into deep models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Mon Dec 03 2018 \n",
      "\n",
      "numpy 1.14.2\n",
      "scipy 1.0.0\n",
      "pomegranate 0.10.0\n",
      "\n",
      "compiler   : GCC 7.2.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-36-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "\n",
    "from pomegranate import *\n",
    "\n",
    "numpy.random.seed(0)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -m -n -p numpy,scipy,pomegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 0.0123739647152\tTime (s): 0.1254\n",
      "Total Improvement: 0.0123739647152\n",
      "Total Time (s): 0.1869\n",
      "[[0.57102217 0.42897783]\n",
      " [0.96611289 0.03388711]\n",
      " [0.2672109  0.7327891 ]\n",
      " [0.00080146 0.99919854]\n",
      " [0.00012695 0.99987305]\n",
      " [0.99900971 0.00099029]\n",
      " [0.7510968  0.2489032 ]\n",
      " [0.99801552 0.00198448]\n",
      " [0.15337515 0.84662485]\n",
      " [0.16009661 0.83990339]]\n",
      "[[0.54443732 0.45556268]\n",
      " [0.6270334  0.3729666 ]\n",
      " [0.15921108 0.84078892]\n",
      " [0.21760758 0.78239242]\n",
      " [0.08559682 0.91440318]\n",
      " [0.39204585 0.60795415]\n",
      " [0.07883944 0.92116056]\n",
      " [0.85338964 0.14661036]\n",
      " [0.35588869 0.64411131]\n",
      " [0.06725062 0.93274938]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class NeuralNetworkWrapper():\n",
    "    def __init__(self, model, i):\n",
    "        self.d = 10\n",
    "        self.model = model\n",
    "        self.i = i\n",
    "        self.model.X = []\n",
    "        self.model.y = []\n",
    "        self.model.w = []\n",
    "    \n",
    "    def log_probability(self, X):\n",
    "        return numpy.log(self.model.predict(X)[:,self.i])\n",
    "    \n",
    "    def summarize(self, X, w):\n",
    "        self.model.X.append(X.copy())\n",
    "        self.model.w.append(w.copy())\n",
    "        \n",
    "        y = numpy.zeros((X.shape[0], 2))\n",
    "        y[:,self.i] = 1\n",
    "        self.model.y.append(y)\n",
    "        \n",
    "    def from_summaries(self, inertia=0.0):\n",
    "        if self.i == 0:\n",
    "            X = numpy.concatenate(self.model.X)\n",
    "            w = numpy.concatenate(self.model.w)\n",
    "            y = numpy.concatenate(self.model.y)\n",
    "        \n",
    "            self.model.train_on_batch(X, y, sample_weight=w)\n",
    "        \n",
    "        self.clear_summaries()\n",
    "    \n",
    "    def clear_summaries(self):\n",
    "        self.model.X = []\n",
    "        self.model.y = []\n",
    "        self.model.w = []\n",
    "        \n",
    "n = 10000\n",
    "\n",
    "X = numpy.random.randn(n, 10)\n",
    "X[::2] += 0.5\n",
    "\n",
    "X_valid = numpy.random.randn(10, 10)\n",
    "X_valid[::2] += 0.5\n",
    "\n",
    "y_init = KMeans(2).fit_predict(X)\n",
    "y = numpy.zeros((X.shape[0], 2))\n",
    "y[numpy.arange(n), y_init] = 1\n",
    "\n",
    "nn = Sequential([Dense(128, input_dim=10, activation='relu'), Dense(2, activation='softmax')])\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "nn.fit(X, y, epochs=1, verbose=0)\n",
    "\n",
    "d1 = NeuralNetworkWrapper(nn, 0)\n",
    "d2 = NeuralNetworkWrapper(nn, 1)\n",
    "\n",
    "model = GeneralMixtureModel([d1, d2])\n",
    "model.fit(X, max_iterations=100, verbose=True)\n",
    "\n",
    "print model.predict_proba(X_valid)\n",
    "\n",
    "model2 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X)\n",
    "print model2.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.85009706, -0.36001267, -0.3307358 ,  1.07383233, -0.41560461,\n",
       "       -0.43283055,  1.48240643, -0.73301856, -1.44795206, -1.16381234])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-65.22359289, -48.00506255, -75.39049114, -59.00727455,\n",
       "       -63.98556007, -55.92679917, -58.53411466, -51.00950827,\n",
       "       -64.21108064, -47.85357562])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.log_probability(X_valid[:10] + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep hidden Markov models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkWrapper():\n",
    "    def __init__(self, model, i):\n",
    "        self.d = 10\n",
    "        self.model = model\n",
    "        self.i = i\n",
    "    \n",
    "    def log_probability(self, X):\n",
    "        return numpy.log(self.model.predict(X)[:,self.i])\n",
    "\n",
    "X = numpy.random.randn(1000, 20, 10)\n",
    "X[:, ::2] += 1\n",
    "\n",
    "y = numpy.zeros((20000, 2))\n",
    "y[::2, 0] = 1\n",
    "y[1::2, 1] = 1\n",
    "\n",
    "nn = Sequential([Dense(128, input_dim=10, activation='relu'), Dense(2, activation='softmax')])\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "nn.fit(X.reshape(20000, 10), y, epochs=10, verbose=0)\n",
    "\n",
    "s1 = State(NeuralNetworkWrapper(nn, 0))\n",
    "s2 = State(NeuralNetworkWrapper(nn, 1))\n",
    "\n",
    "model = HiddenMarkovModel()\n",
    "model.add_states(s1, s2)\n",
    "model.add_transition(model.start, s1, 0.9)\n",
    "model.add_transition(model.start, s2, 0.1)\n",
    "model.add_transition(s1, s1, 0.9)\n",
    "model.add_transition(s1, s2, 0.1)\n",
    "model.add_transition(s2, s1, 0.7)\n",
    "model.add_transition(s2, s2, 0.3)\n",
    "model.bake()\n",
    "\n",
    "model.predict_proba(X[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
